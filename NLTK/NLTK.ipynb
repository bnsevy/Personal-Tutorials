{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.ntlk.org\n",
    "# https://www.tutorialspoint.com/natural_language_toolkit/natural_language_toolkit_tutorial.pdf\n",
    "\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only first time\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word_tokenize module \n",
    "- splits a string into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['On',\n",
       " 'January',\n",
       " '11',\n",
       " ',',\n",
       " '2021',\n",
       " ',',\n",
       " 'Saban',\n",
       " \"'s\",\n",
       " 'Alabama',\n",
       " 'Crimson',\n",
       " 'Tide',\n",
       " 'defeated',\n",
       " 'the',\n",
       " 'Ohio',\n",
       " 'State',\n",
       " 'Buckeyes',\n",
       " '52-24',\n",
       " 'to',\n",
       " 'win',\n",
       " 'the',\n",
       " 'National',\n",
       " 'Championship',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from package import class\n",
    "# from nltk.tokenizer import word_tokenize\n",
    "\n",
    "bama_string = 'On January 11, 2021, Saban\\'s Alabama Crimson Tide defeated the Ohio State Buckeyes 52-24 to win the National Championship.'\n",
    "my_tokens = nltk.tokenize.word_tokenize(bama_string)\n",
    "my_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WordPunctTokenizer class\n",
    "- splits all punctuation into separate tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['On',\n",
       " 'January',\n",
       " '11',\n",
       " ',',\n",
       " '2021',\n",
       " ',',\n",
       " 'Saban',\n",
       " \"'\",\n",
       " 's',\n",
       " 'Alabama',\n",
       " 'Crimson',\n",
       " 'Tide',\n",
       " 'defeated',\n",
       " 'the',\n",
       " 'Ohio',\n",
       " 'State',\n",
       " 'Buckeyes',\n",
       " '52',\n",
       " '-',\n",
       " '24',\n",
       " 'to',\n",
       " 'win',\n",
       " 'the',\n",
       " 'National',\n",
       " 'Championship',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknzr = nltk.tokenize.WordPunctTokenizer()\n",
    "my_tokens = tknzr.tokenize(bama_string)\n",
    "my_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sent_tokenizer module\n",
    "- split text/paragraph into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I took the dog for a walk.',\n",
       " 'After five minutes, it began to rain.',\n",
       " 'We ran back to the house.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para_string = 'I took the dog for a walk. After five minutes, it began to rain. We ran back to the house.'\n",
    "my_tokens = nltk.tokenize.sent_tokenize(para_string)\n",
    "my_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RegexpTokenizer class \n",
    "- gives complete control over how to tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['On',\n",
       " 'January',\n",
       " '11',\n",
       " '2021',\n",
       " \"Saban's\",\n",
       " 'Alabama',\n",
       " 'Crimson',\n",
       " 'Tide',\n",
       " 'defeated',\n",
       " 'the',\n",
       " 'Ohio',\n",
       " 'State',\n",
       " 'Buckeyes',\n",
       " '52',\n",
       " '24',\n",
       " 'to',\n",
       " 'win',\n",
       " 'the',\n",
       " 'National',\n",
       " 'Championship']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example: don't split contradictions like \"Saban's\"\n",
    "tknzr = nltk.tokenize.RegexpTokenizer(\"[\\w']+\")\n",
    "my_tokens = tknzr.tokenize(bama_string)\n",
    "my_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['On',\n",
       " 'January',\n",
       " '11,',\n",
       " '2021,',\n",
       " \"Saban's\",\n",
       " 'Alabama',\n",
       " 'Crimson',\n",
       " 'Tide',\n",
       " 'defeated',\n",
       " 'the',\n",
       " 'Ohio',\n",
       " 'State',\n",
       " 'Buckeyes',\n",
       " '52-24',\n",
       " 'to',\n",
       " 'win',\n",
       " 'the',\n",
       " 'National',\n",
       " 'Championship.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example: always tokenize on whitespace\n",
    "tknzr = nltk.tokenize.RegexpTokenizer('\\s+', gaps = True)\n",
    "my_tokens = tknzr.tokenize(bama_string)\n",
    "my_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TweetTokenizer class \n",
    "- best tokenizer for emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LeBron',\n",
       " 'had',\n",
       " 'himself',\n",
       " 'a',\n",
       " 'night',\n",
       " 'üî•',\n",
       " 'üíØ',\n",
       " \"It's\",\n",
       " 'INSANE',\n",
       " 'that',\n",
       " 'he',\n",
       " '‚Äô',\n",
       " 's',\n",
       " 'doing',\n",
       " 'this',\n",
       " 'at',\n",
       " '36',\n",
       " 'üëè',\n",
       " 'üò§']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_string = 'LeBron had himself a nightüî•üíØ It\\'s INSANE that he‚Äôs doing this at 36üëèüò§'\n",
    "tknzr = nltk.tokenize.TweetTokenizer()\n",
    "my_tokens = tknzr.tokenize(emoji_string)\n",
    "my_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords  \n",
    "Words that are present in text but do not contribute to the meaning of a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# English is one of many languages with a preloaded set of stopwords\n",
    "english_stops = nltk.corpus.stopwords.words('english')\n",
    "# there are too many to show, but listed below are a few examples\n",
    "english_stops[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'student', 'University', 'Switzerland']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example: remove stopwords\n",
    "words = ['I', 'am', 'a', 'student', 'at', 'the', 'University', 'in', 'Switzerland']\n",
    "[word for word in words if word not in english_stops]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
