{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.ntlk.org\n",
    "# https://www.tutorialspoint.com/natural_language_toolkit/natural_language_toolkit_tutorial.pdf\n",
    "# https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only first time\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word_tokenize module \n",
    "- splits a string into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['On',\n",
       " 'January',\n",
       " '11',\n",
       " ',',\n",
       " '2021',\n",
       " ',',\n",
       " 'Saban',\n",
       " \"'s\",\n",
       " 'Alabama',\n",
       " 'Crimson',\n",
       " 'Tide',\n",
       " 'defeated',\n",
       " 'the',\n",
       " 'Ohio',\n",
       " 'State',\n",
       " 'Buckeyes',\n",
       " '52-24',\n",
       " 'to',\n",
       " 'win',\n",
       " 'the',\n",
       " 'National',\n",
       " 'Championship',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from package import class\n",
    "# from nltk.tokenizer import word_tokenize\n",
    "\n",
    "bama_string = 'On January 11, 2021, Saban\\'s Alabama Crimson Tide defeated the Ohio State Buckeyes 52-24 to win the National Championship.'\n",
    "my_tokens = nltk.tokenize.word_tokenize(bama_string)\n",
    "my_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WordPunctTokenizer class\n",
    "- splits all punctuation into separate tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['On',\n",
       " 'January',\n",
       " '11',\n",
       " ',',\n",
       " '2021',\n",
       " ',',\n",
       " 'Saban',\n",
       " \"'\",\n",
       " 's',\n",
       " 'Alabama',\n",
       " 'Crimson',\n",
       " 'Tide',\n",
       " 'defeated',\n",
       " 'the',\n",
       " 'Ohio',\n",
       " 'State',\n",
       " 'Buckeyes',\n",
       " '52',\n",
       " '-',\n",
       " '24',\n",
       " 'to',\n",
       " 'win',\n",
       " 'the',\n",
       " 'National',\n",
       " 'Championship',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknzr = nltk.tokenize.WordPunctTokenizer()\n",
    "my_tokens = tknzr.tokenize(bama_string)\n",
    "my_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sent_tokenizer module\n",
    "- split text/paragraph into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I took the dog for a walk.',\n",
       " 'After five minutes, it began to rain.',\n",
       " 'We ran back to the house.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para_string = 'I took the dog for a walk. After five minutes, it began to rain. We ran back to the house.'\n",
    "my_tokens = nltk.tokenize.sent_tokenize(para_string)\n",
    "my_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RegexpTokenizer class \n",
    "- gives complete control over how to tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['On',\n",
       " 'January',\n",
       " '11',\n",
       " '2021',\n",
       " \"Saban's\",\n",
       " 'Alabama',\n",
       " 'Crimson',\n",
       " 'Tide',\n",
       " 'defeated',\n",
       " 'the',\n",
       " 'Ohio',\n",
       " 'State',\n",
       " 'Buckeyes',\n",
       " '52',\n",
       " '24',\n",
       " 'to',\n",
       " 'win',\n",
       " 'the',\n",
       " 'National',\n",
       " 'Championship']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example: don't split contradictions like \"Saban's\"\n",
    "tknzr = nltk.tokenize.RegexpTokenizer(\"[\\w']+\")\n",
    "my_tokens = tknzr.tokenize(bama_string)\n",
    "my_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['On',\n",
       " 'January',\n",
       " '11,',\n",
       " '2021,',\n",
       " \"Saban's\",\n",
       " 'Alabama',\n",
       " 'Crimson',\n",
       " 'Tide',\n",
       " 'defeated',\n",
       " 'the',\n",
       " 'Ohio',\n",
       " 'State',\n",
       " 'Buckeyes',\n",
       " '52-24',\n",
       " 'to',\n",
       " 'win',\n",
       " 'the',\n",
       " 'National',\n",
       " 'Championship.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example: always tokenize on whitespace\n",
    "tknzr = nltk.tokenize.RegexpTokenizer('\\s+', gaps = True)\n",
    "my_tokens = tknzr.tokenize(bama_string)\n",
    "my_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TweetTokenizer class \n",
    "- best tokenizer for emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LeBron',\n",
       " 'had',\n",
       " 'himself',\n",
       " 'a',\n",
       " 'night',\n",
       " 'üî•',\n",
       " 'üíØ',\n",
       " \"It's\",\n",
       " 'INSANE',\n",
       " 'that',\n",
       " 'he',\n",
       " '‚Äô',\n",
       " 's',\n",
       " 'doing',\n",
       " 'this',\n",
       " 'at',\n",
       " '36',\n",
       " 'üëè',\n",
       " 'üò§']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_string = 'LeBron had himself a nightüî•üíØ It\\'s INSANE that he‚Äôs doing this at 36üëèüò§'\n",
    "tknzr = nltk.tokenize.TweetTokenizer()\n",
    "my_tokens = tknzr.tokenize(emoji_string)\n",
    "my_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords  \n",
    "Words that are present in text but do not contribute to the meaning of a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# English is one of many languages with a preloaded set of stopwords\n",
    "english_stops = nltk.corpus.stopwords.words('english')\n",
    "# there are too many to show, but listed below are a few examples\n",
    "english_stops[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'student', 'University', 'Switzerland']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example: remove stopwords\n",
    "words = ['I', 'am', 'a', 'student', 'at', 'the', 'University', 'in', 'Switzerland']\n",
    "[word for word in words if word not in english_stops]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming  \n",
    "- Technique used to extract the base form of words by removing affixes (root stem)\n",
    "- Looks at form of the word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PorterStemmer class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'write'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_stemmer = nltk.stem.PorterStemmer()\n",
    "word_stemmer.stem('writing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LancasterStemmer class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'writ'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_stemmer = nltk.stem.LancasterStemmer()\n",
    "word_stemmer.stem('written')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RegexpStemmer class  \n",
    "- Takes in a single regular expression, removes any prefix or suffix that matches that expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'writ'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_stemmer = nltk.stem.RegexpStemmer('ing')\n",
    "word_stemmer.stem('ingwriting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SnowballStemmer class  \n",
    "- works with multiple languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bonjour'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "French_stemmer = nltk.stem.SnowballStemmer('french')\n",
    "French_stemmer.stem('Bonjoura')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "- Technique used to extract the base form of words by finding root word\n",
    "- Looks at meaning of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'belief'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "word_lemmatizer.lemmatize('believes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Difference versus stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'believ'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_stemmer = nltk.stem.LancasterStemmer()\n",
    "word_stemmer.stem('believes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### edit_distance(s1, s2)  \n",
    "- Calculates the number of characters that need to be substituted, inserted, or deleted to transform s1 into s2\n",
    "- Possible to weigh subsitution edits differently (default 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = 'Kevin Durant scored 37 points, grabbed 8 rebounds, and had 6 assists in a loss'\n",
    "s2 = 'Stephen Curry scored 30 points, grabbed 4 rebounds, and had 11 assists in a win'\n",
    "\n",
    "edit_distance = nltk.edit_distance(s1, s2)\n",
    "edit_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### edit_proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2179"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_proportion = round(nltk.edit_distance(s1, s2) / len(s1), 4)\n",
    "edit_proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform\n",
      "  - Kevin Durant scored 37 points, grabbed 8 rebounds, and had 6 assists in a loss\n",
      "Into\n",
      "  - Stephen Curry scored 30 points, grabbed 4 rebounds, and had 11 assists in a win\n",
      "     - edit distance: 17\n",
      "     - edit percent: 21.79\n",
      "***************************************************************************\n",
      "Transform\n",
      "  - Stephen Curry scored 30 points, grabbed 4 rebounds, and had 11 assists in a win\n",
      "Into\n",
      "  - Stephen Curry scored 30 points, grabbed 4 rebounds, and had 11 assists in a win\n",
      "     - edit distance: 0\n",
      "     - edit percent: 0.0\n",
      "***************************************************************************\n",
      "Transform\n",
      "  - Javale McGee scored 10 points, and grabbed 5 rebounds in a loss\n",
      "Into\n",
      "  - Stephen Curry scored 30 points, grabbed 4 rebounds, and had 11 assists in a win\n",
      "     - edit distance: 39\n",
      "     - edit percent: 61.9\n",
      "***************************************************************************\n",
      "Transform\n",
      "  - The square root of 49 is 7\n",
      "Into\n",
      "  - Stephen Curry scored 30 points, grabbed 4 rebounds, and had 11 assists in a win\n",
      "     - edit distance: 64\n",
      "     - edit percent: 246.15\n",
      "***************************************************************************\n"
     ]
    }
   ],
   "source": [
    "s3 = 'Javale McGee scored 10 points, and grabbed 5 rebounds in a loss'\n",
    "s4 = 'The square root of 49 is 7'\n",
    "\n",
    "player_list = [s1, s2, s3, s4]\n",
    "target_player = player_list[1]\n",
    "\n",
    "for comparison_player in player_list:\n",
    "    edit_distance = nltk.edit_distance(comparison_player, target_player)\n",
    "    edit_percent = round(100*edit_distance/len(comparison_player), 2)\n",
    "    print('Transform')\n",
    "    print(f'  - {comparison_player}')\n",
    "    print('Into')\n",
    "    print(f'  - {target_player}')\n",
    "    print(f'     - edit distance: {edit_distance}')\n",
    "    print(f'     - edit percent: {edit_percent}')\n",
    "    print('*'*75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **sklearn.feature_extraction.text**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "- The general process of turning a collection of text documents into numerical feature vectors\n",
    "- Tokenization, counting, normalization process\n",
    "- Documents are described by word occurences, while ignoring positional information of the words in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(player_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x26 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 41 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
